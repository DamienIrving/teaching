{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../nci-logo.png)\n",
    "\n",
    "-------\n",
    "# Data Chunking with Dask iPython Notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='part1'></a> \n",
    "## Launch the Jupyter Notebook application\n",
    "\n",
    "#### Using the public hh5 conda environment managed by CLEX\n",
    "\n",
    "Many python modules are available under the hh5 conda environment that is maintained by CLEX, as well as additional modules such as that of CleF used in the previous examples. This environment is publically available and developed to service the CLEX users allowing use cases for the wider communinty.\n",
    "```\n",
    "    $ module use /g/data3/hh5/public/modules\n",
    "    $ module load conda/analysis3\n",
    "```  \n",
    "\n",
    "Launch the Jupyter Notebook application:\n",
    "```\n",
    "    $ jupyter notebook\n",
    "``` \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>NOTE: </b> This will launch the <b>Notebook Dashboard</b> within a new web browser window. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening Multiple Files at Once\n",
    "\n",
    "xarray's `open_mfdataset` allows multiple files to be opened simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /g/data/oi10/replicas/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-CM4/ssp585/r1i1p1f1/day/pr/gr1/v20180701\n",
    "path = '/g/data/oi10/replicas/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-CM4/ssp585/r1i1p1f1/day/pr/gr1/v20180701/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In a similar to way to open_dataset, use open_mfdataset to open the multiple files in `path` and have a look at the Dataset. Do you see anything different from the previous notebook?\n",
    "\n",
    "<a href=\"#ans1\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans1\">\n",
    "<pre><code>\n",
    "f_ssp585 = xr.open_mfdataset(path)\n",
    "f_ssp585\n",
    "</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks\n",
    "\n",
    "Notice that it says:\n",
    "`pr         (time, lat, lon) float32 dask.array<shape=(31390, 180, 288), chunksize=(7300, 180, 288)`\n",
    "There is now an additional component to the shape, and that is `chunksize`.\n",
    "\n",
    "The chunking of the array comes from the integration of Dask with xarray. Dask (see: https://docs.dask.org/en/latest/) is a library for parallel computing. Dask divides the the data array into small pieces called \"chunks\", with each chunk designed to be small enough to fit into memory. \n",
    "\n",
    "In addition to chunking of the array, the file itself may be chunked. Filesystem chunking is available in netCDF-4 and HDF5 datasets. CMIP6 data should all be netCDF-4 and include some form of chunking on the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ncdump -hst '/g/data/oi10/replicas/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-CM4/ssp585/r1i1p1f1/day/pr/gr1/v20180701/pr_day_GFDL-CM4_ssp585_r1i1p1f1_gr1_20150101-20341231.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this case the is chunked such that `pr:_ChunkSizes = 1, 180, 288 ;`\n",
    "\n",
    "Here we see that the data is chunked in time, where one chunk is one time-step and all points in lat-lon.\n",
    "\n",
    "<img src=\"Chunks.png\">\n",
    "image source: https://www.unidata.ucar.edu/blogs/developer/en/entry/chunking_data_why_it_matters\n",
    "\n",
    "Consider 2 types of data access\n",
    "1. Accessing a 2D lat-lon slice in time (RHS figure)\n",
    "2. Accessing a time series at a single lat-lon point (LHS figure)\n",
    "\n",
    "With the chunking above, the first type of data access only needs to access a single chunk, while the second type needs to access ALL the chunks of the data array regardless. This dataset will be fastest for 2D lat-lon single time-step data access.\n",
    "\n",
    "In general, even without chunking - when the data is accessed contiguously (by index order) - time is the slowest variable to access, then y, with x being the fastest. With the chunking method of this CMIP6 dataset, time still remains the slowest variable. For more uniform variable access speeds more evenly spaced chunks would be needed, spacing the chunks in time, lat, and lon.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Time how long it takes to load the precipitation data at `time='2015-01-01'` and then time how long it takes to load the data at `lat=0` and `lon=180` (remember to use `method='nearest'` for the latter case). How much difference is there in the different access methods?\n",
    "\n",
    "<a href=\"#ans4\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans4\">\n",
    "<pre><code>\n",
    "%%time\n",
    "f_ssp585.pr.sel(time='2015-01-01').load()\n",
    "-----------------------------------------------\n",
    "%%time\n",
    "f_ssp585.pr.sel(lat=0,lon=180,method='nearest').load()\n",
    "</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximately the same amount of data took 100x longer to load.\n",
    "\n",
    "The spatial dataset contains 51840 data-points and took order 100ms to load. The time-series dataset has 31390 data-points and took order 10,000ms to load.\n",
    "\n",
    "Chunking and the ways in which the data is read is important in considering both how you access the data and if you wish to parallelise your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF file Chunks versus Dask Chunks\n",
    "\n",
    "Keep in mind, dask chunking is different to chunking of the stored data. As we saw, the stored data is chunked with chunks of size (1,180,288). The Dask array was chunked with size (7300, 180, 288). You can change the chunking in the dask array. In the below example we are specifying that there be 730 chunks in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ssp585 = xr.open_mfdataset(path,chunks={'time':730})\n",
    "f_ssp585"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How big do you make your chunks?\n",
    "\n",
    "The rule of thumb for dask chunks is that you should \"create arrays with a minimum chunksize of at least one million elements\":  http://xarray.pydata.org/en/stable/dask.html#chunking-and-performance\n",
    "\n",
    "netCDF4 chunks are often a lot smaller than dask array chunks. The minimum chunksize exists because if you have too many chunks, then queuing of operations when parallising will be slow, and if they are too big computation and memory can be wasted. The default chunks from dask gave us chunks of size: (7300, 180, 288) or nearly 400 million elements so we could try reducing those chunks if needed. The larger the array, the larger the cost of queueing and the larger chunks may be needed.\n",
    "\n",
    "#### IMPORTANT: Whatever dask array chunks you use, make sure they align with the netCDF4 file chunks!!\n",
    "\n",
    "So far our chunks have been in time, and the netCDF4 file is also chunked in time. If we tried to use dask chunks to optimise the time-series loading of data, it will not help! \n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Try it, load the data with chunks size `(31390,180,1)` (i.e. chunked in lon) and name that file `f_bad_chunk`. Try re-loading the time series of pr at `lat=0` and `lon=180` and time how long it takes.\n",
    "\n",
    "<a href=\"#ans1\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans1\">\n",
    "<pre><code>\n",
    "f_bad_chunk = xr.open_mfdataset(path,chunks={'time':31390,'lat':180,'lon':1})\n",
    "----------------------------------------\n",
    "%%time\n",
    "f_bad_chunk.pr.sel(lat=0,lon=180,method='nearest').load()\n",
    "</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we add some extra cores to the computation?\n",
    "\n",
    "You can easily parallelise xarray code using the dask.distributed.Client and dask array calculations will be run in parrallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "c = Client()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Try running your previous code for `f_bad_chunk` again loading the time series of pr at `lat=0` and `lon=180` and time how long it takes now that there are 4 workers in the dask cluster.\n",
    "\n",
    "Do the same with the original chunking method of `f_ssp585` and see if there is a difference.\n",
    "\n",
    "<a href=\"#ans2\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans2\">\n",
    "<pre><code>\n",
    "%%time\n",
    "f_bad_chunk.pr.sel(lat=0,lon=180,method='nearest').load()\n",
    "----------------------------------------\n",
    "%%time\n",
    "f_ssp585.pr.sel(lat=0,lon=180,method='nearest').load()\n",
    "</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poor chunking with dask can make your performance worse!\n",
    "\n",
    "Both the size of the chunks and the alignment of the chunks with the filesystem chunks are imporant to keep in mind when creating dask chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
