{
 "metadata": {
  "name": "",
  "signature": "sha256:d76f9f41ee737bf246756f83a8cd33f6fb0818b4ce18e92b7f763dd72003af3d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data management"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our previous lessons have shown us how to write programs that ingest a list of data files, perform some calculations on those data, and then print a final result to the screen. While this was a useful exercise in learning the principles of scripting and parsing the command line, in most cases the output of our programs will not be so simple. Instead, programs typically take data as input, manipulate that data, and then output yet more data. Over the course of a multi-year research project, most reseachers will write many different programs that produce many different output datasets. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to:\n",
      "\n",
      "* Manage our data in such as way as to avoid confusion/calamity  \n",
      "\n",
      "Along the way, we will learn:\n",
      "\n",
      "* how to create a Data Reference Syntax\n",
      "* how to view the contents of binary files\n",
      "* about data provenance and metadata"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What's in a name?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this lesson we are going to process some data collected by Australia's Integrated Marine Observing System ([IMOS](http://www.imos.org.au/)).\n",
      "\n",
      "First off, let's load our data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from netCDF4 import Dataset\n",
      "acorn_URL = 'http://thredds.aodn.org.au/thredds/dodsC/IMOS/eMII/demos/ACORN/monthly_gridded_1h-avg-current-map_non-QC/TURQ/2012/IMOS_ACORN_V_20121001T000000Z_TURQ_FV00_monthly-1-hour-avg_END-20121029T180000Z_C-20121030T160000Z.nc.gz'\n",
      "acorn_DATA = Dataset(acorn_URL) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing to notice is the distinctive Data Reference Syntax (DRS) associated with the file. The staff at IMOS have archived the data according to the following directory structure:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`http://thredds.aodn.org.au/thredds/dodsC/<project>/<organisation>/<collection>/<facility>/<data-type>/<site-code>/<year>/`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this we can deduce, without even inspecting the contents of the file, that we have data from the IMOS project that is run by the eMarine Information Infrastructure (eMII). It was collected in 2012 at the Turqoise Coast, Western Australia (TURQ) site of the Australian Coastal Ocean Radar Network (ACORN), which is a network of high frequency radars that measure the ocean surface current. The data type has a sub-DRS of its own, which tells us that the data is a monthly mean product on a regular grid, derived from 1-hourly average current data that has not been quality controlled. It is located in the \"demos\" directory, as the file has been generated for the purpose of providing an example for users at the very helpful [Australian Ocean Data Network](http://portal.aodn.org.au/aodn/) (AODN) [user code library](https://github.com/aodn/imos-user-code-library).\n",
      "\n",
      "Just in case the file gets separated from this informative directory stucture, much of this information is also repeated in the file name itself, along with some more detailed information about the start and end time of the data, and the last time the file was modified."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`<project>_<facility>_V_<time-start>_<site-code>_FV00_<data-type>_<time-end>_<modified>.nc.gz`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the first instance this level of detail seems like a bit of overkill, but consider the scope of the IMOS data archive. It is final resting place for the data collected by the entire national array of observing equipment in Australia, which monitors the open oceans and coastal marine environment covering physical, chemical and biological variables. Since the data are so well labelled, locating all monthly timescale ACORN data from the Turqoise Coast and Rottnest Shelf sites (which represents hundreds of files) would be as simple as typing the following at the command line:      "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`ls */ACORN/monthly_*/{TURQ,ROT}/*/*.nc` "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While it's unlikely that your research will ever involve cataloging data from such a large observational network, it's still a very good idea to develop your own personal DRS for the data you do have. This often involves investing some time at the beginning of a project to think carefully about the design of your directory and file name structures, as these can be very hard to change later on (a good example is [the DRS](http://cmip-pcmdi.llnl.gov/cmip5/docs/cmip5_data_reference_syntax.pdf) used by the Climate Model Intercomparison Project). The combination of bash shell wildcards and a well planned DRS is one of the easiest ways to make your research more efficient and reliable.   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Challenge\n",
      "\n",
      "We haven't even looked inside our IMOS data file and already we have the beginnings of a detailed data management plan. The first step in any research project should be to develop such a plan, so for this challenge we are going to turn back time. If you could start your current research project all over again, what would your data management plan look like? Things to consider include:  \n",
      "\n",
      "* Data Reference Syntax\n",
      "* How long it will take to obtain the data\n",
      "* Storage and backup (here's a [post](http://drclimate.wordpress.com/2013/04/16/backing-up-your-work/) with some backup ideas)\n",
      "\n",
      "Write down and discuss your plan with your partner.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Binary file formats"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can guess from the `.nc` extension that we are dealing with a Network Common Data Form (netCDF) file (it 's also compressed, hence the `.gz`). Had we actually downloaded the file to our computer and uncompressed it, our initial impulse might have been to type, \n",
      "\n",
      "    !cat IMOS_ACORN_V_20121001T000000Z_TURQ_FV00_monthly-1-hour-avg_END-20121029T180000Z_C-20121030T160000Z.nc\n",
      "\n",
      "but such a command would produce an incomprehensible mix symbols and letters. The reason is that up until now, we have been dealing with [text files](http://en.wikipedia.org/wiki/Text_file). These consist of a simple sequence of character data (represented using ASCII, Unicode, or some other standard) separated into lines, meaning that text files are human-readable when opened with a text editor or displayed using `cat`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All other file types (including netCDF) are known collectively as [binary files](http://en.wikipedia.org/wiki/Binary_file). They tend to be smaller and faster for the computer to interpret than text files, but the payoff is that they aren't human-readable unless you have the right intpreter (e.g. `.doc` files aren't readable with your text editor and must instead be opened with Microsoft Word). To view the contents of a netCDF file, we'd need to download (from [here](http://nco.sourceforge.net/)) and install a special command line utility called `ncdump` to view the contents.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this example, however, we haven't actually downloaded the netCDF file to our machine. Instead, IMOS has made the data available via a THREDDS server, which means we can just pass a URL to the `netCDF4.Dataset` function in order to obtain the data. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print acorn_DATA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'netCDF4.Dataset'>\n",
        "root group (NETCDF3_64BIT data model, file format UNDEFINED):\n",
        "    project: Integrated Marine Observing System (IMOS)\n",
        "    Conventions: IMOS version 1.3\n",
        "    institution: Australian Coastal Ocean Radar Network (ACORN)\n",
        "    title: IMOS ACORN Turquoise Coast site (WA) (TURQ), monthly aggregation of one hour averaged current data\n",
        "    instrument: CODAR Ocean Sensors/SeaSonde\n",
        "    site_code: TURQ, Turqoise Coast\n",
        "    ssr_Stations: SeaBird (SBRD), Cervantes (CRVT)\n",
        "    id: IMOS/ACORN/au/TURQ/2012-10-08T15:00:00Z.sea_state\n",
        "    date_created: 2012-10-30T16:31:50Z\n",
        "    abstract: These data have not been quality controlled. The ACORN facility is producing NetCDF files with radials data for each station every ten minutes.  Radials represent the surface sea water state component  along the radial direction from the receiver antenna  and are calculated from the shift of an area under  the bragg peaks in a Beam Power Spectrum.  The radial values have been calculated using software provided  by the manufacturer of the instrument. eMII is using a Matlab program to read all the netcdf files with radial data for two different stations  and produce a one hour average product with U and V components of the current. The final product is produced on a regular geographic (latitude longitude) grid More information on the data processing is available through the IMOS MEST  http://imosmest.aodn.org.au/geonetwork/srv/en/main.home. This file is an aggregation over a month of distinct hourly averaged files.\n",
        "    history: 2012-10-09T03:31:35 Convert totl_TURQ_2012_10_08_1500.tuv to netcdf format using CODAR_Convert_File.\n",
        "2012-10-09T03:31:36 Write CODAR file totl_TURQ_2012_10_08_1500.tuv. Modification of the NetCDF format by eMII to visualise the data using ncWMS \ufffd\n",
        "\t0x0006 %\n",
        "    source: Terrestrial HF radar\n",
        "    keywords: Oceans\n",
        "    netcdf_version: 3.6\n",
        "    naming_authority: Integrated Marine Observing System (IMOS)\n",
        "    quality_control_set: 1\n",
        "    file_version: Level 0 - Raw data\n",
        "    file_version_quality_control: Data in this file has not been quality controlled\n",
        "    geospatial_lat_min: [-29.5386582 -29.5927878 -29.6469169 -29.7010456 -29.7551738 -29.8093016\n",
        " -29.863429  -29.9175559 -29.9716823 -30.0258084 -30.0799339 -30.1340591\n",
        " -30.1881837 -30.242308  -30.2964318 -30.3505551 -30.404678  -30.4588004\n",
        " -30.5129224 -30.5670439 -30.621165  -30.6752857 -30.7294059 -30.7835256\n",
        " -30.8376449 -30.8917637 -30.9458821 -31.        -31.0541175 -31.1082345\n",
        " -31.162351  -31.2164671 -31.2705828 -31.324698  -31.3788127 -31.432927\n",
        " -31.4870408 -31.5411542 -31.5952671 -31.6493795 -31.7034915 -31.7576031\n",
        " -31.8117141 -31.8658247 -31.9199349 -31.9740446 -32.0281538 -32.0822625\n",
        " -32.1363708 -32.1904787 -32.244586  -32.2986929 -32.3527994 -32.4069054\n",
        " -32.4610109]\n",
        "    geospatial_lat_max: [-29.5252994 -29.5794147 -29.6335296 -29.687644  -29.741758  -29.7958715\n",
        " -29.8499845 -29.904097  -29.9582091 -30.0123207 -30.0664318 -30.1205425\n",
        " -30.1746527 -30.2287624 -30.2828717 -30.3369805 -30.3910888 -30.4451966\n",
        " -30.499304  -30.5534109 -30.6075173 -30.6616232 -30.7157287 -30.7698337\n",
        " -30.8239382 -30.8780423 -30.9321458 -30.9862489 -31.0403515 -31.0944536\n",
        " -31.1485553 -31.2026564 -31.2567571 -31.3108573 -31.364957  -31.4190563\n",
        " -31.473155  -31.5272533 -31.5813511 -31.6354484 -31.6895452 -31.7436416\n",
        " -31.7977374 -31.8518328 -31.9059277 -31.960022  -32.0141159 -32.0682094\n",
        " -32.1223023 -32.1763947 -32.2304867 -32.2845781 -32.3386691 -32.3927596\n",
        " -32.4468495]\n",
        "    geospatial_lat_units: degrees_north\n",
        "    geospatial_lon_min: [ 112.3100111  112.3090067  112.3080002  112.3069914  112.3059805\n",
        "  112.3049674  112.3039521  112.3029346  112.3019149  112.3008929\n",
        "  112.2998688  112.2988424  112.2978139  112.2967831  112.29575\n",
        "  112.2947147  112.2936772  112.2926374  112.2915953  112.290551\n",
        "  112.2895044  112.2884556  112.2874044  112.286351   112.2852953\n",
        "  112.2842373  112.283177   112.2821143  112.2810494  112.2799821\n",
        "  112.2789125  112.2778406  112.2767663  112.2756897  112.2746108\n",
        "  112.2735294  112.2724458  112.2713597  112.2702713  112.2691805\n",
        "  112.2680873  112.2669917  112.2658937  112.2647933  112.2636905\n",
        "  112.2625853  112.2614777  112.2603676  112.2592551  112.2581402\n",
        "  112.2570228  112.2559029  112.2547806  112.2536558  112.2525286]\n",
        "    geospatial_lon_max: [ 115.7758038  115.7766744  115.7775469  115.7784212  115.7792975\n",
        "  115.7801756  115.7810557  115.7819376  115.7828215  115.7837073\n",
        "  115.784595   115.7854846  115.7863762  115.7872697  115.7881651\n",
        "  115.7890625  115.7899618  115.7908631  115.7917663  115.7926715\n",
        "  115.7935787  115.7944878  115.7953989  115.796312   115.7972271\n",
        "  115.7981441  115.7990632  115.7999843  115.8009074  115.8018325\n",
        "  115.8027596  115.8036887  115.8046199  115.8055531  115.8064883\n",
        "  115.8074256  115.8083649  115.8093063  115.8102497  115.8111952\n",
        "  115.8121427  115.8130924  115.8140441  115.8149979  115.8159538\n",
        "  115.8169118  115.8178719  115.8188341  115.8197984  115.8207648\n",
        "  115.8217334  115.822704   115.8236768  115.8246518  115.8256289]\n",
        "    geospatial_lon_units: degrees_east\n",
        "    geospatial_vertical_min: 0.0\n",
        "    geospatial_vertical_max: 0.0\n",
        "    geospatial_vertical_units: m\n",
        "    time_coverage_start: 2012-10-01T00:00:00Z\n",
        "    time_coverage_duration: PT1H19M\n",
        "    local_time_zone: 8.0\n",
        "    data_centre_email: info@emii.org.au\n",
        "    data_centre: eMarine Information Infrastructure (eMII)\n",
        "    author: Besnard, Laurent\n",
        "    author_email: laurent.besnard@utas.edu.au\n",
        "    institution_references: http://www.imos.org.au/acorn.html\n",
        "    principal_investigator: Wyatt, Lucy\n",
        "    citation: Citation to be used in publications should follow the format:\"IMOS.[year-of-data-download],[Title],[Data access URL],accessed [date-of-access]\"\n",
        "    acknowledgment: Data was sourced from the Integrated Marine Observing System (IMOS) - IMOS is supported by the Australian Government through the National Collaborative Research Infrastructure Strategy (NCRIS) and the Super Science Initiative (SSI).\n",
        "    distribution_statement: Data may be re-used, provided that related metadata explaining the data has been reviewed by the user, and the data is appropriately acknowledged. Data, products and services from IMOS are provided \"as is\" without any warranty as to fitness for a particular purpose.\n",
        "    comment: These data have not been quality controlled. They represent values calculated using software provided by CODAR Ocean Sensors. The file has been modified by eMII in order to visualise the data using the ncWMS software.This NetCDF file has been created using the IMOS NetCDF User Manual v1.2. A copy of the document is available at http://imos.org.au/facility_manuals.html\n",
        "    data_center: eMarine Information Infrastructure (eMII)\n",
        "    date_modified: 2012-10-30T16:31:50Z\n",
        "    time_coverage_end: 2012-10-29T18:00:00Z\n",
        "    featureType: grid\n",
        "    data_center_email: info@emii.org.au\n",
        "    acknowledgement: Data was sourced from Integrated Marine Observing System (IMOS) - an initiative of the Australian Government being conducted as part of the National Calloborative Research Infrastructure Strategy.\n",
        "    dimensions(sizes): I(55), J(57), TIME(493)\n",
        "    variables(dimensions): float64 \u001b[4mLATITUDE\u001b[0m(I,J), float64 \u001b[4mLONGITUDE\u001b[0m(I,J), int8 \u001b[4mLATITUDE_quality_control\u001b[0m(I,J), int8 \u001b[4mLONGITUDE_quality_control\u001b[0m(I,J), float64 \u001b[4mTIME\u001b[0m(TIME), float32 \u001b[4mSPEED\u001b[0m(TIME,I,J), float32 \u001b[4mUCUR\u001b[0m(TIME,I,J), float32 \u001b[4mVCUR\u001b[0m(TIME,I,J), int8 \u001b[4mTIME_quality_control\u001b[0m(TIME), int8 \u001b[4mSPEED_quality_control\u001b[0m(TIME,I,J), int8 \u001b[4mUCUR_quality_control\u001b[0m(TIME,I,J), int8 \u001b[4mVCUR_quality_control\u001b[0m(TIME,I,J)\n",
        "    groups: \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The great thing about netCDF files is that they contain [metadata](http://en.wikipedia.org/wiki/Metadata) - that is, data about the data. There are global attributes that give information about the file as a whole (shown above - we will come back to these later), while each variable also has its own attributes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The file contains the following variables:'\n",
      "print acorn_DATA.variables.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The file contains the following variables:\n",
        "[u'LATITUDE', u'LONGITUDE', u'LATITUDE_quality_control', u'LONGITUDE_quality_control', u'TIME', u'SPEED', u'UCUR', u'VCUR', u'TIME_quality_control', u'SPEED_quality_control', u'UCUR_quality_control', u'VCUR_quality_control']\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(The 'u' means each variable name is represented by a Unicode string.) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'These are the attributes of the time axis:'\n",
      "print acorn_DATA.variables['TIME']\n",
      "print 'These are some of the time values:'\n",
      "print acorn_DATA.variables['TIME'][0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "These are the attributes of the time axis:\n",
        "<type 'netCDF4.Variable'>\n",
        "float64 TIME(TIME)\n",
        "    standard_name: time\n",
        "    long_name: time\n",
        "    units: days since 1950-01-01 00:00:00\n",
        "    axis: T\n",
        "    valid_min: 0.0\n",
        "    valid_max: 999999.0\n",
        "    _FillValue: -9999.0\n",
        "    calendar: gregorian\n",
        "    comment: Given time lays at the middle of the averaging time period.\n",
        "    local_time_zone: 8.0\n",
        "unlimited dimensions: \n",
        "current shape = (493,)\n",
        "filling off\n",
        "\n",
        "These are some of the time values:\n",
        "[ 22919.          22919.04166667  22919.08333333  22919.125       22919.16666667\n",
        "  22919.20833333  22919.25        22919.29166667  22919.33333333  22919.375     ]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The raw time values are farily meaningless, but we can use the time attributes to convert them to a more meaningful format..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from netCDF4 import num2date\n",
      "units = acorn_DATA.variables['TIME'].units\n",
      "calendar = acorn_DATA.variables['TIME'].calendar\n",
      "\n",
      "times = num2date(acorn_DATA.variables['TIME'], units=units, calendar=calendar)\n",
      "print times[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[datetime.datetime(2012, 10, 1, 0, 0) datetime.datetime(2012, 10, 1, 1, 0)\n",
        " datetime.datetime(2012, 10, 1, 2, 0) datetime.datetime(2012, 10, 1, 3, 0)\n",
        " datetime.datetime(2012, 10, 1, 4, 0) datetime.datetime(2012, 10, 1, 5, 0)\n",
        " datetime.datetime(2012, 10, 1, 6, 0) datetime.datetime(2012, 10, 1, 7, 0)\n",
        " datetime.datetime(2012, 10, 1, 8, 0) datetime.datetime(2012, 10, 1, 9, 0)]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> ### Climate and Forecast (CF) metadata convention\n",
      ">\n",
      "> When performing simple data analysis tasks on netCDF files, command line tools like the Climate Data Operators ([CDO](https://code.zmaw.de/projects/cdo)) are often a better alternative to writing your own functions in Python. However, let's put ourselves in the shoes of the developers of CDO for a minute. In order to convert the time axis to a meaningful list of dates, CDO (and `num2date` for that matter) first needs to identify the units of the time axis. This isn't as easy as you'd think, since the creator of the netCDF file could easily have called the `units` attribute \"`measure`,\" or \"`scale`,\" or something else completely unpredictable instead. They could also have defined the units as \"`weeks since 1-01-01 00:00:00`,\" or \"`milliseconds after 1979-12-31`.\" Obviously what is needed is a standard method for defining netCDF attributes, and that\u2019s where the [Climate and Forecast (CF) metadata convention](http://cf-pcmdi.llnl.gov/) comes in.\n",
      "\n",
      "> The CF metadata standard was first defined back in the early 2000s and has now been adopted by all the major institutions and projects in the weather/climate sciences. There is a nice [blog post](http://drclimate.wordpress.com/2014/06/09/are-you-cf-compliant/) on the topic if you'd like more information, but for the most part you just need to be aware that if a tool like CDO isn't working, it might be because your netCDF file isn't CF compliant."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Calculating the wind speed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_data = v_file.variables['vas'][:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "now both `u_data` and `v_data` are numpy ndarrays, we also check their shape"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(u_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "numpy.ndarray"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u_data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "(3, 28, 26)"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We get the dimension variables as well"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lons = u_file.variables['lon'][:]\n",
      "lats = u_file.variables['lat'][:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and we are going to read the time dimensions and cast it into datetime object using `netcdftime.num2date`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "time = u_file.variables['time'][:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "units = u_file.variables['time'].units"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "calendar"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 89,
       "text": [
        "u'proleptic_gregorian'"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we can now calculate the wind-speed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsp_data = (u_data**2 + v_data**2)**0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`wsp_data` is a `numpy.ndarray`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(wsp_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "numpy.ndarray"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsp_data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 90,
       "text": [
        "(3, 28, 26)"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "example of **writing** a netCDF file using netCDF4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_output_netCDF4(data, varname, time, units, calendar, lats, lons, outfile_name):\n",
      "    \"\"\"\n",
      "    example:\n",
      "\n",
      "    >> write_output_netCDF4(wsp_data, 'wind_speed', time, 'days since 1-01-01\\\n",
      "    ... 00:00:00', 'proleptic_gregorian', lats, lons, 'wsp_data.nc')\n",
      "    \"\"\"\n",
      "\n",
      "    outfile = Dataset(outfile_name, 'w', format='NETCDF4')\n",
      "\n",
      "    # dimensions\n",
      "    outfile.createDimension('time', None)\n",
      "    outfile.createDimension('lat', len(lats))\n",
      "    outfile.createDimension('lon', len(lons))\n",
      "\n",
      "    # variables and their attributes\n",
      "    times = outfile.createVariable('time', 'f8', ('time',))\n",
      "    times.units = units\n",
      "    times.calendar = calendar\n",
      "    latitudes = outfile.createVariable('latitude', 'f4', ('lat',))\n",
      "    latitudes.units = 'degrees_north'\n",
      "    longitudes = outfile.createVariable('longitude', 'f4', ('lon',))\n",
      "    longitudes.units = 'degrees_east'\n",
      "    var = outfile.createVariable(varname, 'f4', ('time', 'lat', 'lon',))\n",
      "\n",
      "    # populates the netcdf variables with numpy arrays\n",
      "    latitudes[:] = lats\n",
      "    longitudes[:] = lons\n",
      "    var[:,:,:] = data\n",
      "    times[:] = time\n",
      "    outfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_output_netCDF4(wsp_data, 'wind_speed', time, 'days since 1-01-01 00:00:00', 'proleptic_gregorian', lats, lons, 'wsp_data.nc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lat = srs_L3P_DATA.variables['lat'][::step]\n",
      "lon = srs_L3P_DATA.variables['lon'][::step]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to have a quick look at the array by using the [matplotlib](http://matplotlib.org/) library, missing values (as defined by the mask) will appear in white by default"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reading the data with cdms2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cdms2\n",
      "\n",
      "u_name = 'uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc'\n",
      "u_file = cdms2.open(u_name)\n",
      "u_data = u_file('uas')\n",
      "\n",
      "v_name = 'vas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc'\n",
      "v_file = cdms2.open(v_name)\n",
      "v_data = v_file('vas')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our two variables, `udata` and `vdata`, are cdms2 transient variables. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'udata is of type:', type(u_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "udata is of type: <class 'cdms2.tvariable.TransientVariable'>\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The nice thing about these transient variables is that they carry the netCDF metadata with them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Metadata about the time axis:' \n",
      "print u_data.getTime()\n",
      "\n",
      "print 'Raw time axis values:'\n",
      "print u_data.getTime()[:]\n",
      "\n",
      "print 'Time axis values in a friendlier format:'\n",
      "print u_data.getTime().asComponentTime()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Metadata about the time axis:\n",
        "   id: time\n",
        "   Designated a time axis.\n",
        "   units:  days since 1-01-01 00:00:00\n",
        "   Length: 3\n",
        "   First:  748548.0\n",
        "   Last:   748609.5\n",
        "   Other axis attributes:\n",
        "      standard_name: time\n",
        "      calendar: proleptic_gregorian\n",
        "      axis: T\n",
        "   Python id:  0x3599190\n",
        "\n",
        "Raw time axis values:\n",
        "[ 748548.   748578.5  748609.5]\n",
        "Time axis values in a friendlier format:\n",
        "[2050-6-16 0:0:0.0, 2050-7-16 12:0:0.0, 2050-8-16 12:0:0.0]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now go ahead and calculate the wind speed,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsp_data = (u_data**2 + v_data**2)**0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and our transient variables are smart enough to pass along the relevant metadata to our new variable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Metadata about the time axis:' \n",
      "print wsp_data.getTime()\n",
      "\n",
      "print 'Time axis values in a friendlier format:'\n",
      "print wsp_data.getTime().asComponentTime()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Metadata about the time axis:\n",
        "   id: time\n",
        "   Designated a time axis.\n",
        "   units:  days since 1-01-01 00:00:00\n",
        "   Length: 3\n",
        "   First:  748548.0\n",
        "   Last:   748609.5\n",
        "   Other axis attributes:\n",
        "      standard_name: time\n",
        "      calendar: proleptic_gregorian\n",
        "      axis: T\n",
        "   Python id:  0x321db50\n",
        "\n",
        "Time axis values in a friendlier format:\n",
        "[2050-6-16 0:0:0.0, 2050-7-16 12:0:0.0, 2050-8-16 12:0:0.0]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data provenance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we go ahead and create a new script (`calc_wind_speed.py`) for calculating the wind speed, there's just one more thing to consider. Looking closely at the global attributes of `uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc` you can see that the entire history of the file, all the way back to its initial download, has been recorded in the `history` attribute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "global_atts = u_file.attributes\n",
      "old_history = global_atts['history']\n",
      "print old_history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Thu Nov 07 14:19:44 2013: cdo sellonlatbox,110,160,-45,-10 uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc\n",
        "Thu Nov 07 14:13:51 2013: cdo seldate,2050-06-01,2050-08-31 uas_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc\n",
        "CMIP5 compliant file produced from raw ACCESS model output using the ACCESS Post-Processor and CMOR2. 2012-03-14T04:40:43Z CMOR rewrote data to comply with CF standards and CMIP5 requirements. Fri Apr 13 12:32:01 2012: corrected model_id from ACCESS1-3 to ACCESS1.3 Fri Apr 13 14:07:50 2012: forcing attribute modified to correct value Wed May  2 13:39:09 2012: updated version number to v20120413.\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last two entries, for instance, were generated by the `cdo` package when it was used to select a temporal (`seldate`) and spatial (`sellonlatbox`) subset of the original data file. This practice of recording the history of the file ensures the [provenance](http://en.wikipedia.org/wiki/Provenance) of the data. In other words, a complete record of everything that has been done to the data is stored with the data, which avoids any confusion in the event that the data is ever moved, passed around to different users, or viewed by its creator many months later.  \n",
      "\n",
      "If we want to create our own entry for the history attribute, we'll need to be able to create a: \n",
      "\n",
      "* Time stamp\n",
      "* Record of what was entered at the command line in order to execute `calc_wind_speed.py`\n",
      "* Method of indicating which verion of the script was run (i.e. because the script is in our git repository)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Time stamp"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A library called `datetime` can be used to find out the time and date right now:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "\n",
      "time_stamp = datetime.datetime.now().strftime(\"%a %b %d %H:%M:%S %Y\")\n",
      "print time_stamp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tue Jun 10 16:59:58 2014\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `strftime` function can be used to customise the appearance of a datetime object; in this case we've made it look just like the other time stamps in our data files. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Command line record"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the Software Carpentry [lesson on command line programs](http://www.software-carpentry.org/v5/novice/python/06-cmdline.html) we met `sys.argv`, which contains all the arguments entered by the user at the command line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "print sys.argv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['-c', '-f', '/home/dbirving/.ipython/profile_default/security/kernel-0f9c6be1-a1d7-4bd9-8f12-c7772ab46070.json', \"--IPKernelApp.parent_appname='ipython-notebook'\", '--profile-dir', '/home/dbirving/.ipython/profile_default', '--parent=1']\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In launching this IPython notebook, you can see that a number of command line arguments were used. To join all these list elements up, we can use the `join` function that belongs to Python strings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "args = \" \".join(sys.argv)\n",
      "print args"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-c -f /home/dbirving/.ipython/profile_default/security/kernel-0f9c6be1-a1d7-4bd9-8f12-c7772ab46070.json --IPKernelApp.parent_appname='ipython-notebook' --profile-dir /home/dbirving/.ipython/profile_default --parent=1\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While this list of arguments is very useful, it doesn't tell us which Python installation was used to execute those arguments. The `sys` library can help us out here too:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exe = sys.executable\n",
      "print exe "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/local/uvcdat/1.5.1/bin/python\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Git hash"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the Software Carpentry [lessons on git](http://www.software-carpentry.org/v5/novice/git/index.html) we learned that each commit is associated with a unique 40-character identifier known as a hash. We can use the git Python library to get the hash associated with the script:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from git import Repo\n",
      "import os\n",
      "\n",
      "git_hash = Repo(os.getcwd()).head.commit.hexsha\n",
      "print git_hash"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2db1f92b517fe4262f4c8d98d2ec8b9b4c89b154\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now put all this information together for our history entry: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entry = \"\"\"%s: %s %s (Git hash: %s)\"\"\" %(time_stamp, exe, args, git_hash[0:7])\n",
      "print entry"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tue Jun 10 16:59:58 2014: /usr/local/uvcdat/1.5.1/bin/python -c -f /home/dbirving/.ipython/profile_default/security/kernel-0f9c6be1-a1d7-4bd9-8f12-c7772ab46070.json --IPKernelApp.parent_appname='ipython-notebook' --profile-dir /home/dbirving/.ipython/profile_default --parent=1 (Git hash: 2db1f92)\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Putting it all together"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we've been experimenting in the IPython notebook to familiarise ourselves with UV-CDAT and the other Python modules that might be useful for calculating the wind speed. We should now go ahead and write a script, so we can repeat the process with a single entry at the command line:  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat calc_wind_speed.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "import os, sys\r\n",
        "import datetime\r\n",
        "from git import Repo\r\n",
        "\r\n",
        "import cdms2\r\n",
        "cdms2.setNetcdfShuffleFlag(0)\r\n",
        "cdms2.setNetcdfDeflateFlag(0)\r\n",
        "cdms2.setNetcdfDeflateLevelFlag(0)\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    script = sys.argv[0]\r\n",
        "    u_file = sys.argv[1]\r\n",
        "    u_var = sys.argv[2]\r\n",
        "    v_file = sys.argv[3]\r\n",
        "    v_var = sys.argv[4]\r\n",
        "    outfile_name = sys.argv[5]\r\n",
        "    \r\n",
        "    u_data, ufile_atts = read_data(u_file, u_var)\r\n",
        "    v_data, vfile_atts = read_data(v_file, v_var)\r\n",
        "    \r\n",
        "    wsp_data = calc_wsp(u_data, v_data) \r\n",
        "    \r\n",
        "    write_output(wsp_data, ufile_atts, outfile_name)    \r\n",
        "\r\n",
        "\r\n",
        "def read_data(ifile, var):\r\n",
        "    \"\"\"Read data from ifile corresponding to the var variable\"\"\"\r\n",
        "    \r\n",
        "    fin = cdms2.open(ifile)\r\n",
        "    data = fin(var)\r\n",
        "    file_atts = fin.attributes \r\n",
        "    fin.close()\r\n",
        "    \r\n",
        "    return data, file_atts\r\n",
        "\r\n",
        "\r\n",
        "def calc_wsp(uwnd, vwnd):\r\n",
        "    \"\"\"Calculate the wind speed and create relevant attributes\"\"\"\r\n",
        "    \r\n",
        "    wsp = (uwnd**2 + vwnd**2)**0.5\r\n",
        "    \r\n",
        "    wsp.id = 'wsp'\r\n",
        "    wsp.long_name = 'Wind speed'\r\n",
        "    wsp.units = 'm s-1'\r\n",
        "    \r\n",
        "    return wsp\r\n",
        "\r\n",
        "\r\n",
        "def write_output(wsp_data, ufile_atts, outfile_name):\r\n",
        "    \"\"\"Write the output file\"\"\"\r\n",
        "    \r\n",
        "    outfile = cdms2.open(outfile_name, 'w')\r\n",
        "        \r\n",
        "    new_history = create_history()\r\n",
        "    old_history = ufile_atts['history']\r\n",
        "\r\n",
        "    setattr(outfile, 'history', \"\"\"%s\\n%s\"\"\" %(new_history, old_history))\r\n",
        "    for att_name in ufile_atts.keys():\r\n",
        "        if att_name != \"history\":  # history excluded because we've already done it\r\n",
        "            setattr(outfile, att_name, ufile_atts[att_name])\r\n",
        "\r\n",
        "    outfile.write(wsp_data)\r\n",
        "    outfile.close()\r\n",
        "\r\n",
        "\r\n",
        "def create_history():\r\n",
        "    \"\"\"Create the new entry for the global history file attribute\"\"\"\r\n",
        "    \r\n",
        "    time_stamp = datetime.datetime.now().strftime(\"%a %b %d %H:%M:%S %Y\")\r\n",
        "    exe = sys.executable\r\n",
        "    args = \" \".join(sys.argv)\r\n",
        "    git_hash = Repo(os.getcwd()).head.commit.hexsha\r\n",
        "\r\n",
        "    return \"\"\"%s: %s %s (Git hash: %s)\"\"\" %(time_stamp, exe, args, git_hash[0:7])\r\n",
        "\r\n",
        "\r\n",
        "main()\r\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(The `cdms2.setNetcdf...` commands simply specify that we want the classic netCDF format - see [this post](http://drclimate.wordpress.com/2014/06/09/are-you-cf-compliant/) for more details on netCDF formats.)\n",
      "\n",
      "We can now run this script at the command line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!/usr/local/uvcdat/1.5.1/bin/python calc_wind_speed.py uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc uas vas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc vas wsp_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The finished product"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now inspect the attributes in our new file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ncdump -h wsp_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "netcdf wsp_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus {\r\n",
        "dimensions:\r\n",
        "\ttime = UNLIMITED ; // (3 currently)\r\n",
        "\tbound = 2 ;\r\n",
        "\tlat = 28 ;\r\n",
        "\tlon = 26 ;\r\n",
        "variables:\r\n",
        "\tdouble time(time) ;\r\n",
        "\t\ttime:bounds = \"time_bnds\" ;\r\n",
        "\t\ttime:units = \"days since 1-01-01 00:00:00\" ;\r\n",
        "\t\ttime:standard_name = \"time\" ;\r\n",
        "\t\ttime:calendar = \"proleptic_gregorian\" ;\r\n",
        "\t\ttime:axis = \"T\" ;\r\n",
        "\tdouble time_bnds(time, bound) ;\r\n",
        "\tdouble lat(lat) ;\r\n",
        "\t\tlat:bounds = \"lat_bnds\" ;\r\n",
        "\t\tlat:units = \"degrees_north\" ;\r\n",
        "\t\tlat:long_name = \"latitude\" ;\r\n",
        "\t\tlat:standard_name = \"latitude\" ;\r\n",
        "\t\tlat:axis = \"Y\" ;\r\n",
        "\tdouble lat_bnds(lat, bound) ;\r\n",
        "\tdouble lon(lon) ;\r\n",
        "\t\tlon:bounds = \"lon_bnds\" ;\r\n",
        "\t\tlon:modulo = 360. ;\r\n",
        "\t\tlon:long_name = \"longitude\" ;\r\n",
        "\t\tlon:standard_name = \"longitude\" ;\r\n",
        "\t\tlon:units = \"degrees_east\" ;\r\n",
        "\t\tlon:axis = \"X\" ;\r\n",
        "\t\tlon:topology = \"circular\" ;\r\n",
        "\tdouble lon_bnds(lon, bound) ;\r\n",
        "\tfloat wsp(time, lat, lon) ;\r\n",
        "\t\twsp:associated_files = \"baseURL: http://cmip-pcmdi.llnl.gov/CMIP5/dataLocation gridspecFile: gridspec_atmos_fx_ACCESS1-3_rcp85_r0i0p0.nc\" ;\r\n",
        "\t\twsp:long_name = \"Wind speed\" ;\r\n",
        "\t\twsp:standard_name = \"eastward_wind\" ;\r\n",
        "\t\twsp:cell_methods = \"time: mean\" ;\r\n",
        "\t\twsp:units = \"m s-1\" ;\r\n",
        "\t\twsp:missing_value = 1.e+20f ;\r\n",
        "\t\twsp:history = \"2012-03-14T04:40:42Z altered by CMOR: Treated scalar dimension: \\'height\\'. 2012-03-14T04:40:42Z altered by CMOR: replaced missing value flag (-1.07374e+09) with standard missing value (1e+20).\" ;\r\n",
        "\r\n",
        "// global attributes:\r\n",
        "\t\t:Conventions = \"CF-1.4\" ;\r\n",
        "\t\t:history = \"Tue Jun 10 17:00:00 2014: /usr/local/uvcdat/1.5.1/bin/python calc_wind_speed.py uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc uas vas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc vas wsp_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc (Git hash: 2db1f92)\\n\",\r\n",
        "\t\t\t\"Thu Nov 07 14:19:44 2013: cdo sellonlatbox,110,160,-45,-10 uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc\\n\",\r\n",
        "\t\t\t\"Thu Nov 07 14:13:51 2013: cdo seldate,2050-06-01,2050-08-31 uas_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc\\n\",\r\n",
        "\t\t\t\"CMIP5 compliant file produced from raw ACCESS model output using the ACCESS Post-Processor and CMOR2. 2012-03-14T04:40:43Z CMOR rewrote data to comply with CF standards and CMIP5 requirements. Fri Apr 13 12:32:01 2012: corrected model_id from ACCESS1-3 to ACCESS1.3 Fri Apr 13 14:07:50 2012: forcing attribute modified to correct value Wed May  2 13:39:09 2012: updated version number to v20120413.\" ;\r\n",
        "\t\t:initialization_method = 1 ;\r\n",
        "\t\t:CDI = \"Climate Data Interface version 1.5.6 (http://code.zmaw.de/projects/cdi)\" ;\r\n",
        "\t\t:product = \"output\" ;\r\n",
        "\t\t:creation_date = \"2012-03-14T04:40:43Z\" ;\r\n",
        "\t\t:frequency = \"mon\" ;\r\n",
        "\t\t:references = \"See http://wiki.csiro.au/confluence/display/ACCESS/ACCESS+Publications\" ;\r\n",
        "\t\t:title = \"ACCESS1-3 model output prepared for CMIP5 RCP8.5\" ;\r\n",
        "\t\t:experiment = \"RCP8.5\" ;\r\n",
        "\t\t:realization = 1 ;\r\n",
        "\t\t:project_id = \"CMIP5\" ;\r\n",
        "\t\t:institute_id = \"CSIRO-BOM\" ;\r\n",
        "\t\t:model_id = \"ACCESS1.3\" ;\r\n",
        "\t\t:parent_experiment_id = \"historical\" ;\r\n",
        "\t\t:experiment_id = \"rcp85\" ;\r\n",
        "\t\t:cmor_version = \"2.8.0\" ;\r\n",
        "\t\t:parent_experiment = \"historical\" ;\r\n",
        "\t\t:modeling_realm = \"atmos\" ;\r\n",
        "\t\t:branch_time = 732311. ;\r\n",
        "\t\t:institution = \"CSIRO (Commonwealth Scientific and Industrial Research Organisation, Australia), and BOM (Bureau of Meteorology, Australia)\" ;\r\n",
        "\t\t:version_number = \"v20120413\" ;\r\n",
        "\t\t:forcing = \"GHG, Oz, SA, Sl, Vl, BC, OC, (GHG = CO2, N2O, CH4, CFC11, CFC12, CFC113, HCFC22, HFC125, HFC134a)\" ;\r\n",
        "\t\t:CDO = \"Climate Data Operators version 1.5.6.1 (http://code.zmaw.de/projects/cdo)\" ;\r\n",
        "\t\t:physics_version = 1 ;\r\n",
        "\t\t:contact = \"The ACCESS wiki: http://wiki.csiro.au/confluence/display/ACCESS/Home. Contact Tony.Hirst@csiro.au regarding the ACCESS coupled climate model. Contact Peter.Uhe@csiro.au regarding ACCESS coupled climate model CMIP5 datasets.\" ;\r\n",
        "\t\t:table_id = \"Table Amon (01 February 2012) 01388cb4507c2f05326b711b09604e7e\" ;\r\n",
        "\t\t:tracking_id = \"724f536a-c5fa-4a68-85f1-ff277af34c75\" ;\r\n",
        "\t\t:parent_experiment_rip = \"r1i1p1\" ;\r\n",
        "}\r\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since most of the file attributes were inherited by default from the input data file (i.e. the u-wind file), it's worth checking to see if there are any that don't make sense. Sure enough, the standard name is misleading:\n",
      "\n",
      "`wsp:standard_name = \"eastward_wind\"`\n",
      "\n",
      "We should revise our script so that it removes or renames the standard name, but beyond that we should resist the urge to start cleaning up. The `associated_files` wind speed attribute, for instance, makes little sense to anyone who isn't involved in the CMIP5 project. While this might seem like a reasonable argument for deleting that attribute, once an attribute is deleted it's gone forever. The `associated_files` attribute is taking up a negligible amount of memory, so why not just leave it there just in case? When in doubt, keep metadata."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Challenge\n",
      "\n",
      "Does your data management plan from the first challenge adequately address this issue of data provenance? If not, go ahead and add to your plan now. Things to consider include:\n",
      "\n",
      "* How to capture and store metadata relating to output files that aren't self describing (i.e. unlike `.nc` files, formats like `.csv` or `.png` don't store things like global and variable attributes within them) \n",
      "\n",
      "Discuss the additions you've made to your plan with your partner."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}